---
title             : "Trusting but forgetting impressive science"
shorttitle        : "Trusting but forgetting impressive science"
header-includes:  | # to prevent floats from moving past certain points (for the appendix)
  \usepackage{placeins} 
  
author: 
  - name          : "Jan Pfänder"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "janlukas.pfaender@gmail.com"
    #role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
  - name          : "Sophie de Rouilhan"
    affiliation   : "2"
    #role:         
  - name          : "Hugo Mercier"
    affiliation   : "2"
    email         : "hugo.mercier@gmail.com"
    address: " "
    
affiliation:
    
  - id            : "1"
    institution   : "Swiss Federal Institute of Aquatic Science and Technology (Eawag), Department of Environmental Social Sciences"

  - id            : "2"
    institution   : "Institut Jean Nicod, Département d’études cognitives, ENS, EHESS, PSL University, CNRS, France"    
            
authornote: |
  We thank Kevin Chen for excellent research assistance.
  
abstract: |
  
  Cultural beliefs and practices often spread because they appeal to existing cognitive mechanisms. Science, however, appears to be an exception. Scientific concepts are highly counterintuitive, and people know very little about science, yet it is among the most trusted institutions. Here, we test a cognitive model of trust in science that is compatible with these observations: the rational impression account. According to this account, people trust scientists because they are impressed by their findings, and this impression persists after specific knowledge has vanished. We present evidence for this model in two experiments (total n = 696) with UK participants. In Experiment 1, exposure to more impressive scientific findings led participants to think of the relevant scientists as more competent and their scientific discipline as more trustworthy. In Experiment 2, we show that participants have these impressions despite almost immediately forgetting relevant content.

keywords          : Trust in science; science knowledge; cognition; deficit model
floatsintext      : yes
linenumbers       : no 
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "doc" # "doc" for nice look, "man" for manuscripty
output: 
  papaja::apa6_pdf: 
    keep_tex: true
  papaja::apa6_docx: default
always_allow_html: true
appendix:
  - "appendix_gpt-prompt.Rmd"
  - "appendix_above-median-recall.Rmd"
  - "appendix_stability.Rmd"
  - "appendix_inter-coder.Rmd"
  - "appendix_exp1.Rmd"
  - "appendix_exp2.Rmd"
  - "appendix_exp3.Rmd"
  - "appendix_exp4.Rmd"
bibliography: references.bib
---

```{r setup, include=FALSE}
# Figure out output format
is_docx <- knitr::pandoc_to("docx") | knitr::pandoc_to("odt")
is_latex <- knitr::pandoc_to("latex")
is_html <- knitr::pandoc_to("html")

# Word-specific things
table_format <- ifelse(is_docx, "huxtable", "kableExtra")  # Huxtable tables
conditional_dpi <- ifelse(is_docx, 300, 300)  # Higher DPI
conditional_align <- ifelse(is_docx, "default", "center")  # Word doesn't support align

# Knitr options
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE,
  # tidy.opts = list(width.cutoff = 120),  # Code width
  # fig.retina = 3, dpi = conditional_dpi,
  # fig.width = 7, fig.asp = 0.618,
  #fig.align = conditional_align, out.width = "100%",
  fig.path = "output/figures/",
  cache.path = "output/_cache/",
  fig.process = function(x) {  # Remove "-1" from figure names
    x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
    if (file.rename(x, x2)) x2 else x
  },
  options(scipen = 99999999)  # Prevent scientific notation
)

# R options
options(
  width = 90,  # Output width
  dplyr.summarise.inform = FALSE,  # Turn off dplyr's summarize() auto messages
  knitr.kable.NA = "",  # Make NAs blank in kables
  kableExtra.latex.load_packages = FALSE,  # Don't add LaTeX preamble stuff
  modelsummary_factory_default = table_format,  # Set modelsummary backend
  modelsummary_format_numeric_latex = "plain"  # Don't use siunitx
)

```

```{r packages, include=FALSE}
# load required packages
library("papaja")      # For APA style manuscript   
library("lme4")        # model specification / estimation
library("lmerTest")    # provides p-values in the output
library("tidyverse")   # data wrangling and visualisation
library("afex")        # anova and deriving p-values from lmer
library("broom")       # extracting data from model fits 
library("broom.mixed") # extracting data from mixed models
library("metafor")     # doing mata analysis
library("patchwork")   # put several plots together
library("ggridges")    # for plots
library("gghalves")    # for plots
library("ggbeeswarm")  # Special distribution-shaped point jittering
library("knitr")       # for tables
library("kableExtra")  # also for tables
library("ggpubr")      # for combining plots with ggarrange() 
library("grid")        # for image plots   
library("gridExtra")   # for image plots
library("png")         # for image plots
library("modelsummary") # for regression tables
library(readxl)
library(ggrepel)  # For better label placement
library(wesanderson) # colours
```

```{r functions}
# load plot theme
source("functions/plot_theme.R") 

# load other functions
source("functions/functions.R")
```

# Introduction

```{r}
# read gss data
gss <- read_csv("gss_cross/data/gss_cross_cleaned.csv")


# Average confidence levels by institution and year
gss_summary_numeric <- gss |>
  group_by(year, institution, institution_label) |>
  summarize(mean_confidence = mean(confidence, na.rm = TRUE)) |> 
  ungroup() |> 
  # remove years without measure
  drop_na(mean_confidence )


# Share of people with great deal of trust by institution and year
gss_summary_great_deal <- gss |>
  drop_na(confidence) |> 
  group_by(year, institution, institution_label) |>
  summarize(
    total_responses = n(),
    great_deal_count = sum(confidence_factor == "A great deal confidence", na.rm = TRUE),
    share = great_deal_count / total_responses,
    # remove 0s
    share = ifelse(share == 0, NA, share)
  ) |>
  ungroup() |> 
  # remove years without measure
  drop_na(share)

# check the four on average top trusted institutions
top_trusted_institutions <- gss |>
  group_by(institution, institution_label) |>
  summarise(mean_confidence = mean(confidence, na.rm = TRUE)) |>
  ungroup() |> 
  arrange(desc(mean_confidence)) |>
  slice_head(n = 4)  # Select top 4

# check the four top institutions with highest share of great deal of trust
top_intitutions_great_deal <- gss_summary_great_deal |> 
  group_by(institution, institution_label) |> 
  summarize(mean_share = mean(share, na.rm=TRUE)) |> 
  ungroup() |> 
  arrange(desc(mean_share)) |>
  # Select top 4
  slice_head(n = 4) |> 
  rounded_numbers() |> 
  mutate(mean_share = paste0(round(mean_share * 100), "%")) %>% 
  split(.$institution)

# religion share of great deal of trust
religion_great_deal <- gss_summary_great_deal |> 
  group_by(institution, institution_label) |> 
  summarize(mean_share = mean(share, na.rm=TRUE)) |> 
  ungroup() |> 
  filter(institution %in% c("conclerg")) |> 
  mutate(mean_share = paste0(round(mean_share * 100), "%")) %>% 
  split(.$institution)

# get the number of institutions
n_institutions <- gss |> 
  distinct(institution, institution_label) |> 
  nrow()
```

Scholars have suggested that culturally widespread beliefs often owe their success to their ability to tap into existing cognitive mechanisms [for a general exposition, see @sperberExplainingCultureNaturalistic1996]. This idea has been applied, for instance, to religion [@boyerNaturalnessReligiousIdeas1994; @lawsonRethinkingReligionConnecting1990], fiction [@gottschallStorytellingAnimalHow2012; @dubourgWhyImaginaryWorlds2022], music [@fitchBiologyEvolutionMusic2006], rituals [@lienardWhenceCollectiveRituals2006], or medical practices [@mitonUniversalCognitiveMechanisms2015]. In each case, cognitive mechanisms render some cultural elements more likely to draw our attention, be remembered, or be passed along. When a cultural element violates the expectations raised by our cognitive mechanisms, it is supposed to be only in a minimal manner that maximises attraction or memorization, as in the case minimally counter-intuitive religious beliefs [@boyerNaturalnessReligiousIdeas1994; @norenzayanMemoryMysteryCultural2006].

In these models of culture, it is the ability to produce cultural elements that people find appealing that would largely explain the status of their creators, from successful fiction authors to religious figures [see, e.g., @andreCulturalEvolutionProducers2023; @boyerDerivingFeaturesReligions2021; @singhCulturalEvolutionShamanism2018]. In this context, science seems to stand out as an exception. By contrast with fiction, which people (in the US) consume for several hours a day on average, consumption of science-related materials (news, popular science, etc.) is minimal, with only a small minority consuming scientific content several times a week [@funkScienceNewsInformation2017]. By contrast with the “naturalness of religious ideas" [@boyerNaturalnessReligiousIdeas1994], many researchers have pointed out the “unnatural nature of science,” [@wolpertUnnaturalNatureScience1994; see @cromerUncommonSenseHeretical1995; @shtulmanScienceblindWhyOur2017; @mccauleyWhyReligionNatural2011a]: how counterintuitive many fundamental scientific concepts are, from Newton’s first law of motion [which violates our intuitions about physics, @mccloskeyCurvilinearMotionAbsence1980], to evolution by natural selection [which violates, inter alia, our intuitions about reproduction, @ronfardInhibitingIntuitionScaffolding2021]. Given that most people do not consume much scientific content, it is not surprising that average levels of science knowledge are very low [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. What *is* surprising is that, in spite of this lack of knowledge, and of the “unnaturalness” of much of science, people worldwide tend to trust science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @colognaTrustScientistsTheir2025]. In the US, where survey data allows for comparison, science is among the most trusted institutions [@funkPublicConfidenceScientists2020].

We start by briefly reviewing existing models of trust in science, pointing out that most do not have much to say about the cognitive foundations of trust in science, being chiefly interested in explaining mistrust of science. One of these models, the deficit model, offers a potential cognitive explanation of trust in science, in that trust in science would be grounded in an understanding and knowledge of science. However, as mentioned above, this is difficult to reconcile with the fact that trust in science tends to be much higher than understanding or knowledge of science. The recently developed rational impression account of trust in science [@pfanderRationalImpressionAccount2025a] can reconcile high levels of trust in science with lack of science knowledge and understanding. This article offers a direct experimental test of two central tenets of this rational impression account: (i) that people trust impressive science, and (ii) that they promptly forget what had impressed them.

# Theoretical background

Much of the literature on public trust in science has focused on explaining why some people do not trust science [@bauerWhatCanWe2007]. The deficit model assumes that trust deficits among the public are due to a lack of scientific knowledge [@sturgisScienceSocietyReEvaluating2004]. Motivated reasoning accounts argue that people reject science to maintain coherence with certain prior beliefs and worldviews [@lewandowskyWorldviewmotivatedRejectionScience2021]. Other psychological accounts highlight ideologies, conspiratorial thinking, or personality as root causes of mistrust [@hornseyAttitudeRootsJiu2017a]. In the sociological literature, the alienation model suggests that mistrust in science is only one symptom of a broader alienation of people from modern institutions [@gauchatCulturalAuthorityScience2011]. These accounts do not attempt to explain the elevated level of public trust in science. Implicitly, they take trust in science for granted, as a normatively rational default, and focus on explaining deviations from this default.

Recently, a large survey project in 68 countries found trust in scientists to be "moderately high" across all countries (mean = 3.62; sd= 0.70; Scale: 1 = very low, 2 = somewhat low, 3 = neither high nor low, 4 = somewhat high, 5 = very high), with not a single country below midpoint trust [@colognaTrustScientistsTheir2025]. This is in line with past global surveys, which found that most people trust science at least to some extent [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020]. In the US, the General Social Survey (GSS) allows to compare trust in science with trust in other institutions. Combining data from 1973 to 2022, on average `r top_intitutions_great_deal$consci$mean_share` of Americans say they have a great deal of confidence in the scientific community, the second highest score among `r n_institutions` institutions, and surpassing religion by far (`r religion_great_deal$conclerg$mean_share`)[^1]. These surveys might still underestimate public trust in scientific knowledge: A recent study in the US found that participants almost always accepted the scientific consensus on basic, non-politicized knowledge questions (e.g. 'Are electrons are smaller than atoms?'), even those participants who said they do not trust science and who believed anti-science conspiracy theories, e.g. that the earth is flat [@pfanderQuasiuniversalAcceptanceBasic2025b].

[^1]: Numbers are based on our own calculations using on the publicly available GSS cumulative data

While existing accounts focus on explaining mistrust, the deficit model implicitly offers an explanation for trust: if a lack of knowledge is the main reason for a lack of trust, then knowledge should be the main cause of trust. The issue with this explanation is that, while people tend to trust science, they do not know much about it [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. A recent global study on 68 countries found no association between the countries' Program for International Student Assessment (PISA) scores, and the national average trust in scientists [@colognaTrustScientistsTheir2025]. A seminal meta analysis found that science knowledge, more generally, was only weakly correlated to attitudes towards science [@allumScienceKnowledgeAttitudes2008].

Recently, a cognitive explanation for public trust in science has been proposed: the rational impression account [@pfanderRationalImpressionAccount2025a]. According to this account, people do not need a profound understanding or detailed knowledge of science, to rationally perceive it as trustworthy. Instead, people can come to this conclusion by using basic cognitive mechanisms of information evaluation. The rational impression account comprises three such mechanisms: First, in many situations, we infer accuracy from consensus—i.e. if something is highly consensual, it is likely to be true. The literature on the wisdom of crowds has shown that this inference is often appropriate [see e.g., @hastieRobustBeautyMajority2005]. In non-science related contexts, it has been shown that people go even further and infer that convergent answers are likely accurate, and those who made them competent, even if they had no priors about their competence[@pfanderHowWiseCrowd2025].

Second, we infer competence from possessing rare knowledge—if someone knows something that is difficult to know, we are impressed, and deem that individual competent. People perceive others who share valuable ideas as more competent [@altayItMyIdea2020]. Outside the realm of science, with trivia questions, it has been shown that people have accurate perceptions of whether something is hard to know or not, and that they use this information to infer someone’s competence [@dubourgUsingNestedStructure2025].

Third, we are likely to forget the specific knowledge that generated our impressions of competence. Relevant evidence comes from memory research: Some research has shown that implicit memory is more stable than explicit memory [e.g., @slomanForgettingPrimedFragment1988; @parkinDifferentialNatureImplicit1990]. Other research has argued that memory encodes information both as "verbatim" details (exact words or numbers) and "gist" representations [the essence or bottom line meaning; @reynaScientificTheoryGist2021], and that the verbatim details tend to fade faster than the gist [@murphyForgettingVerbatimInformation1994]. As an extreme example, patients with severe amnesia can continue to experience emotions linked to events they could not recall [@feinsteinSustainedExperienceEmotion2010]. Similarly, in the case of science, people might forget the specific content of science knowledge (explicit memory, or verbatim details) they have been exposed to, but retain an abstract impression of scientists' trustworthiness (implicit memory, or gist). While this process has not been tested, evidence suggest that impression formation and knowledge retention can be quite detached: @liquinMotivatedLearnAccount2022 found that people find some science-related explanations more satisfying than others. However, the satisfaction people felt for an explanation did not predict how well they could recall it shortly after [@liquinMotivatedLearnAccount2022].

The rational impression account of trust in science can reconcile high levels of trust in science with the fact that people aren’t very interested in science (it’s enough that they have been exposed to science at school, even if they forget much of what they learnt), and that science is counterintuitive (a finding doesn’t have to be intuitive to be impressive—to some extent, the contrary could be true, provided we believe in the finding). Moreover, it grounds trust in science in the operation of well-established cognitive mechanisms. If the general operation of these mechanisms has been studied, their specific application to scientific information and trust in science has not.

Here, we test two key predictions from the rational impression account: People trust more science they perceive as more impressive (H1, tested in Experiment 1), and they forget most of what had impressed them (H2, tested in Experiment 2).

# The present studies

```{r exp1}
# Analyze data of experiments and store results

# Experiment 1

# read the cleaned version data set
exp1 <- read_csv("experiment_2/data/cleaned.csv")

# Hypotheses

# H1 a
H1a_model <- lmer(competence ~ impressiveness + (1 | id), exp1)

# H1 b
H1b_model <- lmer(competence ~ impressed + (1 | id), exp1)

# H2 a
H2a_model <- lmer(trust ~ impressiveness + (1 | id), exp1)

# H2 b
H2b_model <- lmer(trust ~ impressed + (1 | id), exp1)

# Research questions

# RQ1
RQ1_model <- lmer(learn ~ impressiveness + (1 | id), exp1)

# RQ2
# for H1
RQ2_H1a_model <- lmer(trust ~ impressiveness*consensus + (1 | id), exp1)
RQ2_H1b_model <- lmer(trust ~ impressed*consensus + (1 | id), exp1)
# for H2
RQ2_H2a_model <- lmer(competence ~ impressiveness*consensus + (1 | id), exp1)
RQ2_H2b_model <- lmer(competence~ impressed*consensus + (1 | id), exp1)

# Manipulation check
manipulation_check_model <- lmer(impressed ~ impressiveness + (1 | id), exp1) 

# by discipline
manipulation_check_archeo <- lm(impressed ~ impressiveness, 
                                  exp1 %>% filter(discipline == "archeo")) 

manipulation_check_entom <- lm(impressed ~ impressiveness, 
                                  exp1 %>% filter(discipline == "entom")) 

# extract descriptives for inline reporting
exp1_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp1$id),
  gender = exp1 %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp1 %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                  .names = "{.fn}")) %>% rounded_numbers(),
  # Means
  means = exp1 %>% 
    group_by(impressiveness) %>% 
    summarize(across(c(competence, trust, impressed, learn), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    mutate_if(is.numeric, round, digits = 2) %>% 
    split(.$impressiveness),
  means_by_discipline = exp1 %>% 
    group_by(impressiveness, discipline) %>% 
    summarize(across(c(competence, trust, impressed, learn), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    mutate_if(is.numeric, round, digits = 2) %>% 
    super_split(impressiveness, discipline),
  # H1
  H1a = text_ready(H1a_model),
  H1b = text_ready(H1b_model),
  # H2
  H2a = text_ready(H2a_model),
  H2b = text_ready(H2b_model),
  # RQ1
  RQ1 = text_ready(RQ1_model),
  # RQ2
  RQ2_H1a = text_ready(RQ2_H1a_model), 
  RQ2_H1b = text_ready(RQ2_H1b_model), 
  RQ2_H2a = text_ready(RQ2_H2a_model), 
  RQ2_H2b = text_ready(RQ2_H2b_model),
  # Manipulation check
  manipulation_check = text_ready(manipulation_check_model),
  manipulation_check_archeo = text_ready(manipulation_check_archeo),
  manipulation_check_entom = text_ready(manipulation_check_entom)
)
```

```{r exp2}
# Experiment 2

# read the cleaned version data set
exp2 <- read_xlsx("experiment_4/data/experiment_cleaned.xlsx")

exp2_above_median <- read_xlsx("experiment_4/data/experiment_cleaned_above_median.xlsx")

exp2_validation <- read_xlsx("experiment_4/data/validation_cleaned.xlsx")

# Compute ICC 
coder_comparison <- read_csv("experiment_4/data/coder_comparison.csv")

ICC_raw_output <- coder_comparison %>%
  filter(knowledge_dim == "totalp") %>%
  estICCs(Y = "rating", subjects = "subject_id", raters = "coder", estimator = "MLE") 

# extracting ICC for agreement between single ratings of random raters 
ICC <- ICC_raw_output$ICCs |> 
  rownames_to_column("measure") |> 
  filter(measure == "ICCa1") |> 
  rounded_numbers()

# Average Inter-coder agreement between chat gpt and human coders
coder_agreement <- read_csv("experiment_4/data/coder_agreement.csv")

agreement_human_gpt <- coder_agreement |> 
  pivot_longer(ends_with("GPT"), 
               names_to = "coder_pair", 
               values_to = "agreement") |> 
  summarize(mean_agreement = mean(agreement)) |> 
  # make nicer output
  mutate(mean_agreement = paste0(round(mean_agreement, digits = 1), "%")) |> 
  pull()

agreement_human <- paste0(round(mean(coder_agreement$agr_c1_c2), digits = 1), "%")

# Hypotheses

# initialize an empty results data frame
results <- data.frame(
  hypothesis = character(),
  test_type = character(),
  statistic = numeric(),
  p_value = numeric(),
  mean_difference = numeric(),
  stringsAsFactors = FALSE
)

# Attention! Sampe = all Participants for H1

# H1a: One-sample t-test/Wilcoxon signed-rank test for change in competence
if (shapiro.test(exp2$change_competence)$p.value > 0.05) {
  h1a <- t.test(exp2$change_competence, mu = 0)
  results <- rbind(results, data.frame(
    hypothesis = "H1a",
    test_type = "one sample t-test",
    statistic = h1a$statistic,
    p_value = h1a$p.value,
    mean_difference = h1a$estimate
  ), row.names = NULL)
} else {
  h1a <- wilcox.test(exp2$change_competence, mu = 0, exact = FALSE)
  results <- rbind(results, data.frame(
    hypothesis = "H1a",
    test_type = "Wilcoxon signed-rank test",
    statistic = h1a$statistic,
    p_value = h1a$p.value,
    mean_difference = NA
  ), row.names = NULL)
}

# H1b: One-sample t-test/Wilcoxon signed-rank test for change in trust
if (shapiro.test(exp2$change_trust)$p.value > 0.05) {
  h1b <- t.test(exp2$change_trust, mu = 0)
  results <- rbind(results, data.frame(
    hypothesis = "H1b",
    test_type = "one sample t-test",
    statistic = h1b$statistic,
    p_value = h1b$p.value,
    mean_difference = h1b$estimate
  ), row.names = NULL)
} else {
  h1b <- wilcox.test(exp2$change_trust, mu = 0, exact = FALSE)
  results <- rbind(results, data.frame(
    hypothesis = "H1b",
    test_type = "Wilcoxon signed-rank test",
    statistic = h1b$statistic,
    p_value = h1b$p.value,
    mean_difference = NA
  ), row.names = NULL)
}

# Attention! Sampe = above median recall score participants for H2 and H3

# H2: Test forgetting score against zero
if (shapiro.test(exp2_above_median$forgetting_score)$p.value > 0.05) {
  h2 <- t.test(exp2_above_median$forgetting_score, mu = 0)
  results <- rbind(results, data.frame(
    hypothesis = "H2",
    test_type = "one sample t-test",
    statistic = h2$statistic,
    p_value = h2$p.value,
    mean_difference = h2$estimate
  ), row.names = NULL)
} else {
  h2 <- wilcox.test(exp2_above_median$forgetting_score, mu = 0, exact = FALSE)
  results <- rbind(results, data.frame(
    hypothesis = "H2",
    test_type = "Wilcoxon signed-rank test",
    statistic = h2$statistic,
    p_value = h2$p.value,
    mean_difference = NA
  ), row.names = NULL)
}

# H3: One-sample test for impressive forgetting score against zero
if (shapiro.test(exp2_above_median$impressive_forgetting_score)$p.value > 0.05) {
  h3 <- t.test(exp2_above_median$impressive_forgetting_score, mu = 0)
  results <- rbind(results, data.frame(
    hypothesis = "H3",
    test_type = "one sample t-test",
    statistic = h3$statistic,
    p_value = h3$p.value,
    mean_difference = h3$estimate
  ), row.names = NULL)
} else {
  h3 <- wilcox.test(exp2_above_median$impressive_forgetting_score, mu = 0, exact = FALSE)
  results <- rbind(results, data.frame(
    hypothesis = "H3",
    test_type = "Wilcoxon signed-rank test",
    statistic = h3$statistic,
    p_value = h3$p.value,
    mean_difference = NA
  ), row.names = NULL)
}

# H4a: Independent t-test for text impressiveness
h4a <- t.test(
  impressiveness ~ condition,
  data = exp2_validation
)
results <- rbind(results, data.frame(
  hypothesis = "H4a",
  test_type = "independent t-test",
  statistic = h4a$statistic,
  p_value = h4a$p.value,
  mean_difference = h4a$estimate[[1]] - h4a$estimate[[2]]
))

# H4b: Independent t-test for competence change
h4b <- t.test(
  competence ~ condition,
  data = exp2_validation
)
results <- rbind(results, data.frame(
  hypothesis = "H4b",
  test_type = "independent t-test",
  statistic = h4b$statistic,
  p_value = h4b$p.value,
  mean_difference = h4b$estimate[[1]] - h4b$estimate[[2]]
))

# H4c: Independent t-test for trust change
h4c <- t.test(
  trust ~ condition,
  data = exp2_validation
)
results <- rbind(results, data.frame(
  hypothesis = "H4c",
  test_type = "independent t-test",
  statistic = h4c$statistic,
  p_value = h4c$p.value,
  mean_difference = h4c$estimate[[1]] - h4c$estimate[[2]]
))

# Clean result data frame
results <- results %>% 
  # report p.value according to apa standards
  mutate(p.value = case_when(p_value < 0.001 ~ "< .001",
                             TRUE ~ sprintf("= %.3f", p_value)
  )
  ) %>% 
  # round all other terms
  rounded_numbers()

# extract descriptives for inline reporting for all participants
exp2_descriptives_everyone <- list(
  # Demographics
  n_subj = n_distinct(exp2$id),
  gender = exp2 %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp2 %>%
    summarize(
      across(
        age2,
        list(
          mean = ~mean(.x, na.rm = TRUE),
          median = ~median(.x, na.rm = TRUE),
          sd = ~sd(.x, na.rm = TRUE)
        ),
        .names = "{.fn}"
      )
    ) %>%
    rounded_numbers(),
  # Means (raw scales)
  descriptive_stats = exp2 %>% 
    summarize(across(c("knowledge_rawscore", "competence", "change_competence", 
                       "trust", "change_trust", "knowledge_score",
                       "forgetting_score", "n_impressive", 
                       "impressive_knowledge_rawscore", "impressive_knowledge_score", 
                       "impressive_forgetting_score"),
                     list(median = ~median(.x, na.rm = TRUE), 
                          mean = ~mean(.x, na.rm = TRUE), 
                          sd = ~sd(.x, na.rm = TRUE)), 
                     .names = "{.col}_{.fn}")) %>%
    mutate_if(is.numeric, round, digits = 2), 
  # Hypotheses Results, 
  results = results %>%
    # select only results pertaining to main experiment
    filter(hypothesis %in% c("H1a", "H1b")) %>%
    split(.$hypothesis)
)

# extract descriptives for inline reporting for above median participants
exp2_descriptives_above_median <- list(
  # Demographics
  n_subj = n_distinct(exp2_above_median$id),
  gender = exp2_above_median %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp2_above_median %>%
    summarize(
      across(
        age2,
        list(
          mean = ~mean(.x, na.rm = TRUE),
          median = ~median(.x, na.rm = TRUE),
          sd = ~sd(.x, na.rm = TRUE)
        ),
        .names = "{.fn}"
      )
    ) %>%
    rounded_numbers(),
  # Means (raw scales)
  descriptive_stats = exp2_above_median %>% 
    summarize(across(c("knowledge_rawscore", "competence", "change_competence", 
                       "trust", "change_trust", "knowledge_score",
                       "forgetting_score", "n_impressive", 
                       "impressive_knowledge_rawscore", "impressive_knowledge_score", 
                       "impressive_forgetting_score"),
                     list(median = ~median(.x, na.rm = TRUE), 
                          mean = ~mean(.x, na.rm = TRUE), 
                          sd = ~sd(.x, na.rm = TRUE)), 
                     .names = "{.col}_{.fn}")) %>%
    mutate_if(is.numeric, round, digits = 2), 
  # How many elements did participants find impressive 
  # (by discipline since n varied between disciplines) 
  n_impressive = exp2_above_median %>% 
    group_by(condition) %>% 
    summarize(across(c("n_impressive"),
                     list(median = ~median(.x, na.rm = TRUE), 
                          mean = ~mean(.x, na.rm = TRUE), 
                          sd = ~sd(.x, na.rm = TRUE)), 
                     .names = "{.col}_{.fn}")) %>%
    mutate_if(is.numeric, round, digits = 2) %>%
    split(.$condition),
  # Hypotheses Results, 
  results = results %>%
    # select only results pertaining to main experiment
    filter(hypothesis %in% c("H2", "H3")) %>%
    split(.$hypothesis)
)

# extract descriptives for evaluation study
exp2_descriptives_validation <- list(
  # Demographics
  n_subj = n_distinct(exp2_validation$id),
  gender = exp2_validation %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp2_validation %>%
    summarize(
      across(
        age,
        list(
          mean = ~mean(.x, na.rm = TRUE),
          median = ~median(.x, na.rm = TRUE),
          sd = ~sd(.x, na.rm = TRUE)
        ),
        .names = "{.fn}"
      )
    ) %>%
    rounded_numbers(),
  # Means (raw scales)
  descriptive_stats = exp2_validation %>% 
    summarize(across(c("competence", "trust", "impressiveness"), 
                     list(median = ~median(.x, na.rm = TRUE), 
                          mean = ~mean(.x, na.rm = TRUE), 
                          sd = ~sd(.x, na.rm = TRUE)), 
                     .names = "{.col}_{.fn}"))%>%
    mutate_if(is.numeric, round, digits = 2), 
  by_condition = exp2_validation %>% 
    group_by(condition) %>% 
    summarize(across(c("competence", "trust", "impressiveness"), 
                     list(median = ~median(.x, na.rm = TRUE), 
                          mean = ~mean(.x, na.rm = TRUE), 
                          sd = ~sd(.x, na.rm = TRUE)), 
                     .names = "{.col}_{.fn}"))%>%
    mutate_if(is.numeric, round, digits = 2) %>% 
    split(.$condition), 
  # Hypotheses Results, 
  results = results %>%
    # select only results pertaining to main experiment
    filter(hypothesis %in% c("H4a", "H4b", "H4c")) %>%
    split(.$hypothesis)
)
```

```{r combined-data}
# combine data of all three studies

variables <- c("study", "id", "impressed", "competence", "trust", "discipline", "impressiveness")

combined_data <- bind_rows(exp1 %>% 
                             mutate(study = "Experiment 1", 
                                    # use clearer value labels
                                    impressiveness = ifelse(impressiveness == "basic", "basic", "impressive")) |> 
                             select(any_of(variables)), 
                           exp2 %>% 
                             mutate(study = "Experiment 2 (recall)", 
                                    impressiveness = "impressive") |> 
                             # use naming coherent with Experiment 1
                             rename(impressed = global_impressiveness) |> 
                             select(any_of(variables)), 
                           exp2_validation %>% 
                             mutate(study = "Experiment 2 (evaluation)", 
                                    condition = ifelse(condition == "recall", "recall", "impressive")) |> 
                             # use naming coherent with Experiment 1
                             rename(impressed = impressiveness, 
                                    impressiveness = condition) |> 
                             select(any_of(variables))
                           ) |>  
  # make a unique id variable
  mutate(id = paste0("study", study, "_", id), 
         study = factor(study, levels = c("Experiment 1", 
                                          "Experiment 2 (recall)",
                                          "Experiment 2 (evaluation)"))
         )
```

(ref:overview-plot) **A.** An overview of the differences in trust, according to whether the participants had read a basic, an impressive, or another participant's recalled version of the impressive vignette. The density plots show the distributions of participants' trust scores. The dots represent model estimates, and the vertical bars the 95% confidence intervals of the model predictions. **B.** The distribution of knowledge scores (ranging from 0% to 100% retained information) in the recall study of Experiment 2. Each dot corresponds to one participant. For the boxplot, the box represents the interquartile range (IQR), that is, the distance between the first and third quartiles, the center line indicates the median, and the outer lines (whiskers) extend to 1.5 times the IQR or the most extreme values within this range.

```{r overview-plot, fig.cap="(ref:overview-plot)"}
# make a recall plot

# Define colors
fox_palette <- wes_palette("FantasticFox1")

recall_plot <- exp2 |> 
  ggplot(aes(x = 0, y = knowledge_score)) +  
  # Half-boxplot
  geom_half_boxplot(side = "l", width = 0.1, outlier.shape = NA, 
                    alpha = 0.7, fill = fox_palette[2], color = fox_palette[2]) +  
  # Jittered points
  geom_half_point(side = "l", transformation = position_jitter(width = 0.05, height = 0), 
                  alpha = 0.7, color = fox_palette[2]) +  
  # Small dotted tick mark at maximum knowledge score
  geom_segment(aes(x = -0.5, xend = 0.1, y = 1, yend = 1), 
               color = fox_palette[5], linewidth = 0.6, linetype = "dotted") +  
  # Label for maximum knowledge score
  annotate("text", x = -0.5, y = 1.02, label = "Perfect recall", 
           hjust = 0, vjust = 0, color = fox_palette[5], size = 3.5) + 
  labs(y = "Recall Score", x = NULL) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1.05)) +  
  plot_theme +
  theme(axis.text.x = element_blank(),   # Removes x-axis text labels
        axis.ticks.x = element_blank(),  # Removes x-axis ticks
        axis.title.x = element_blank())  # Removes x-axis title



# get model predictions

# for Experiment 1
intercept_exp1 <- tidy(H2a_model) |> 
  select(estimate) |> 
  slice(1) |> 
  pull()

predictions_exp1 <- H2a_model |> 
  tidy(conf.int = TRUE) |> 
  mutate(impressiveness = ifelse(term == "(Intercept)", "basic", "impressive"), 
      # make sure to add the value of the intercept to have the predicted value
      # instead of the differential
      across(c(estimate, conf.low, conf.high), ~ifelse(term == "(Intercept)", .x, 
                                                       .x + intercept_exp1)), 
      study = "Experiment 1") |> 
  filter(effect == "fixed") |> 
  select(study, impressiveness, estimate, conf.low, conf.high)

# for Experiment 2 recall
predictions_exp2_recall <- combined_data |>
  filter(study == "Experiment 2 (recall)") |> 
  group_by(study, impressiveness) |> 
  summarise(
    estimate = mean(trust, na.rm = TRUE),
    se_trust = sd(trust, na.rm = TRUE) / sqrt(n()),  # Standard Error
    conf.low = estimate - 1.96 * se_trust,  # 95% CI Lower Bound
    conf.high = estimate + 1.96 * se_trust   # 95% CI Upper Bound
  ) |> 
  select(-se_trust)

# for Experiment 2 evaluation 
# instead of using the t-test from h4c, we will run a linear regression (in order to get a prediction for the intercept)
h4c_as_linear_model <- lm(trust ~ condition, exp2_validation)

intercept_exp2_evaluation <- tidy(h4c_as_linear_model ) |> 
  select(estimate) |> 
  slice(1) |> 
  pull()

predictions_exp2_evaluation <- h4c_as_linear_model |> 
  tidy(conf.int = TRUE) |> 
  mutate(impressiveness = ifelse(term == "(Intercept)", "impressive", "recall"), 
         # make sure to add the value of the intercept to have the predicted value
         # instead of the differential
         across(c(estimate, conf.low, conf.high), ~ifelse(term == "(Intercept)", .x, 
                                                          .x + intercept_exp2_evaluation )), 
         study = "Experiment 2 (evaluation)") |> 
  select(study, impressiveness, estimate, conf.low, conf.high)


predictions <- bind_rows(predictions_exp1, 
                         predictions_exp2_recall,
                         predictions_exp2_evaluation) |>  
  # have study being a factor
  mutate(study = factor(study, levels = c("Experiment 1", 
                                          "Experiment 2 (recall)",
                                          "Experiment 2 (evaluation)"))
         )

# Plot
impressiveness_plot <- combined_data |> 
  ggplot(aes(x = impressiveness, y = trust, fill = study, color = study)) +   
  geom_half_violin(side = "r", nudge = 0.05, alpha = 0.5, adjust = 0.6) + 
  geom_pointrange(data = predictions, 
                  aes(x = impressiveness, y = estimate, color = study, 
                      ymin = conf.low, ymax = conf.high), 
                  position = position_dodge(width = 0.7), 
                  alpha = 1
  ) +
  scale_fill_manual(values = wes_palette("FantasticFox1")) +  
  scale_color_manual(values = wes_palette("FantasticFox1")) +  
  labs(x = NULL, y = "Trust", fill = "Impressiveness", color = "Impressiveness") +  # Remove x-axis title
  plot_theme +
  theme(
    legend.position = "top",           # Position the legend at the top of the plot
    legend.justification = "left",   # Right-align the legend
    legend.title = element_blank()    # Remove the legend title
  ) + 
  guides(colour = guide_legend(nrow = 2), 
         fill = guide_legend(nrow = 2))

# combined plot
combined_plot <- impressiveness_plot + recall_plot +
  plot_annotation(
    tag_levels = 'A'
  ) +
  plot_layout(
    widths = c(0.6, 0.4)  # Adjust the ratio of the two plots
  )

combined_plot
```

Both experiments were preregistered, and the choice of sample size was informed by power simulations. The preregistrations, along with all materials, data, and code, can be found on Open Science Framework [project page](https://osf.io/j3bk4/overview) (<https://osf.io/j3bk4/overview>). All analyses were conducted in R (version 4.2.2) using R Studio. We used two-sided tests for all hypotheses. Unless mentioned otherwise, we report unstandardized estimates that can be interpreted in units of the original scales.

As part of this project, we conducted two additional experiments which we present in detail in the ESM. The first experiment ('Experiment 1b' in the ESM) is almost identical to Experiment 1, but suffered from a minor technical error during the implementation. Nonetheless, its findings are identical to those of Experiment 1. The second experiment ('Experiment 2b') was supposed to test the same hypothesis as Experiment 2, but the treatment—a very short distraction task—did not alter any of the outcome variables, suggesting that either the distraction task was too short, or that our outcome measure—a set of multiple-choice questions—was not sufficiently sensitive.

# Experiment 1

The goal of Experiment 1 was to test whether exposure to impressive science content enhanced people’s trust in scientists and their discipline. Participants were presented with vignettes about scientific findings in the disciplines of entomology and archaeology. The impressiveness of the texts was manipulated by creating one ‘basic’ and one ‘impressive’ version for each of the disciplines (see Table \@ref(tab:exp1-stimuli)). Impressiveness was manipulated within participants, but between disciplines: each participant was randomly assigned to see an impressive version for one discipline, and a basic version for the other discipline. We tested the following hypotheses:

H1a: After having read an impressive text about a discipline’s findings, compared to when reading a basic text, participants perceive that discipline’s scientists as more competent.

H1b: Across both conditions, participants who are more impressed by the text about a discipline also tend to perceive the scientists of that discipline as more competent.

H2a: After reading an impressive text about a discipline’s findings, compared to when reading a basic text, participants will trust the discipline more.

H2b: Across both conditions, participants who are more impressed by the text about a discipline also tend to trust the discipline more.

The results of two research questions, about perceptions of learning and of consensus, are reported in the ESM.

## Methods

### Participants

A power simulation (see OSF) suggested that the minimum required sample size to detect a statistically significant effect for all hypotheses with a power of 0.9 is 100 participants. We therefore recruited 100 participants from the UK via Prolific. One participant failed the attention check, resulting in a final sample of `r exp1_descriptives$n_subj` participants (`r exp1_descriptives$gender$female$n` female, `r exp1_descriptives$gender$male$n` male; $age_\text{mean}$: `r exp1_descriptives$age$mean`, $age_\text{sd}$: `r exp1_descriptives$age$sd`, $age_\text{median}$: `r exp1_descriptives$age$median`).

### Procedure

After providing their consent to participate in the study and passing an attention check (see ESM), participants read a short introductory text, and then two vignettes (one basic and one impressive) about scientific findings in the disciplines of entomology and archaeology, in a randomized order. After reading each vignette, participants were asked: “How much do you feel you’ve learnt about [human history/insects] by reading this text?” [1 - Nothing, 2 - A bit, 3 - Some, 4 - Quite a bit, 5 - A lot]); “How impressive do you think the findings of the [archaeologists/entomologists] described in the text are?” [1 - Not very impressive, 2 - A bit impressive, 3 - Quite impressive, 4 - Very impressive, 5 - Extremely impressive]); “Would you agree that reading this text has made you think of [archaeologists/entomologists] as more competent than you thought before?” [1 - Strongly disagree, 2 - Disagree, 3 - Neither agree nor disagree, 4 - Agree, 5 - Strongly agree]); and “Having read this text, would you agree that you trust the discipline of [archaeology/entomology] more than you did before?” [1 - Strongly disagree, 2 - Disagree, 3 - Neither agree nor disagree, 4 - Agree, 5 - Strongly agree]. Finally, we asked: “To which extent do you think the findings from the short text you just read reflect a minority or a majority opinion among archaeologists?” [1 - Small minority, 2 - Minority, 3 - About half, 4 - Majority, 5 - Large majority].

### Materials

Table \@ref(tab:exp1-stimuli) shows the stimuli used in Experiment 1.

```{r exp1-stimuli}
# Create a matrix of image file paths as Markdown-formatted strings
table <- data.frame(discipline = c("Archeology", 
                                   "Entomology"),
                    impressive = c("Archaeologists, scientists who study human history and prehistory, are able to tell, from their bones, whether someone was male or female, how old they were, and whether they suffered from a range of diseases. Archaeologists can now tell at what age someone, dead for tens of thousands of years, stopped drinking their mother’s milk, from the composition of their teeth.
Archaeologists learn about the language that our ancestors or cousins might have had. For instance, the nerve that is used to control breathing is larger in humans than in apes, plausibly because we need more fine-grained control of our breathing in order to speak. As a result, the canal containing that nerve is larger in humans than in apes – and it is also enlarged in Neanderthals.
Archaeologists can also tell, from an analysis of the tools they made, that most Neanderthals were right-handed. It’s thought that handedness is related to the evolution of language, another piece of evidence suggesting that Neanderthals likely possessed a form of language.", 
"Entomologists are the scientists who study insects. Some of them have specialized in understanding how insects perceive the world around them, and they have uncovered remarkable abilities. 

Entomologists interested in how flies’ visual perception works have used special displays to present images for much less than the blink of an eye, electrodes to record how individual cells in the flies’ brain react, and ultra-precise electron microscopy to examine their eyes. Thanks to these techniques, they have shown that some flies can perceive images that are displayed for just three milliseconds (a thousandth of a second) – about ten times shorter than a single movie frame (of which there are 24 per second). 

Entomologists who study the hair of crickets have shown that these microscopic hairs, which can be found on antenna-like organs attached to the crickets’ rear, are maybe the most sensitive organs in the animal kingdom. The researchers used extremely precise techniques to measure how the hair reacts to stimuli, such as laser-Doppler velocimetry, a technique capable of detecting the most minute of movements. They were able to show that the hair could react to changes in the motion of the air that had less energy than one particle of light, a single photon."),
                    basic = c("Archaeology is the science that studies human history and prehistory based on the analysis of objects from the past such as human bones, engravings, constructions, and various objects, from nails to bits of pots. This task requires a great deal of carefulness, because objects from the past need to often be dug out from the ground and patiently cleaned, without destroying them in the process.

Archaeologists have been able to shed light on human history in all continents, from ancient Egypt to the Incas in Peru or the Khmers in Cambodia.

Archaeologists study the paintings made by our ancestors, such as those that can be found in Lascaux, a set of caves found in the south of France that have been decorated by people at least 30000 years ago.

Archaeologists have also found remains of our more distant ancestors, showing that our species is just one among several that appeared, and then either changed or went extinct, such as Neanderthals, Homo erectus, or Homo habilis.", 
"Entomologists are scientists who investigate insects, typically having a background in biology. They study, for example, how a swarm of bees organizes, or how ants communicate with each other. 

They also study how different insects interact with each other and their environment, whether some species are in danger of going extinct, or whether others are invasive species that need to be controlled.

Sometimes entomologists study insects by observing them in the wild, sometimes they conduct controlled experiments in laboratories, to see for example how different environmental factors change the behavior of insects, or to track exactly the same insects over a longer period of time.

An entomologist often specializes in one type of insect in order to study it in depth. For example, an entomologist who specializes in ants is called a myrmecologist. "))

# Output the table
if (knitr::is_latex_output() || knitr::is_html_output()) {
  # For LaTeX or HTML: Use kable, column_spec, and kable_styling
  table_output <- kableExtra::kable(table,
                  col.names = c("", "Impressive", "Basic"),
                  caption = "Stimuli of Experiment 1",
                  align = "l",
                  booktabs = TRUE,
                  longtable = TRUE,
                  #full_width = TRUE,
                  #format = "pandoc"
                  ) %>%
  kable_styling(latex_options = "repeat_header",
                font_size = 8) %>%
  column_spec(1, width = "5em") %>%
    column_spec(2, width = "25em") %>%
    column_spec(3, width = "25em")
} else {
  # For Word: Use flextable and rename columns
  table_output <- flextable::flextable(table |> 
                                         rename(`Impressive text` = impressive, 
                                                `Basic text` = basic, 
                                                `Discipline` = discipline)) %>%
    flextable::set_caption(caption = "Stimuli of Experiment 1") %>%
    flextable::autofit() %>%
    flextable::width(j = 1, width = 2) %>%
    flextable::width(j = 2, width = 3.5) %>%
    flextable::width(j = 3, width = 3.5) %>%
  flextable::fontsize(size = 10) 
}

table_output

```

## Results and discussion

As a manipulation check, we find that participants perceived the impressive texts to be more impressive (mean = `r exp1_descriptives$means$imp$impressed_mean`, sd = `r exp1_descriptives$means$imp$impressed_sd`; $\hat{b}$ = `r exp1_descriptives$manipulation_check$impressivenessimp$estimate` `r exp1_descriptives$manipulation_check$impressivenessimp$ci`, p `r exp1_descriptives$manipulation_check$impressivenessimp$p.value`) than the basic texts (mean = `r exp1_descriptives$means$basic$impressed_mean`, sd = `r exp1_descriptives$means$basic$impressed_sd`).

Participants perceived scientists as more competent after having read an impressive text (H1a: $\hat{b}_{\text{Competence}}$ = `r exp1_descriptives$H1a$impressivenessimp$estimate` `r exp1_descriptives$H1a$impressivenessimp$ci`, p `r exp1_descriptives$H1a$impressivenessimp$p.value`; mean = `r exp1_descriptives$means$imp$competence_mean`, sd = `r exp1_descriptives$means$imp$competence_sd`) than after having read a basic one (mean = `r exp1_descriptives$means$basic$competence_mean`, sd = `r exp1_descriptives$means$basic$competence_sd`). Pooled across both conditions, participants' impressiveness ratings were positively associated with competence: When participants reported being more impressed, they evaluated scientist's as more competent (H1b: $\hat{b}$ = `r exp1_descriptives$H1b$impressed$estimate` `r exp1_descriptives$H1b$impressed$ci`, p `r exp1_descriptives$H1b$impressed$p.value`). Figure \@ref(fig:overview-plot) provides an overview of the main results of both Experiment 1, and Experiment 2.

Participants also trusted a discipline more after having read an impressive text (H2a: $\hat{b}_{\text{trust}}$ = `r exp1_descriptives$H2a$impressivenessimp$estimate` `r exp1_descriptives$H2a$impressivenessimp$ci`, p `r exp1_descriptives$H2a$impressivenessimp$p.value`; mean = `r exp1_descriptives$means$imp$trust_mean`, sd = `r exp1_descriptives$means$imp$trust_sd`) than after having read a basic one (mean = `r exp1_descriptives$means$basic$trust_mean`, sd = `r exp1_descriptives$means$basic$trust_sd`). Participants' impressiveness ratings were positively associated with trust when pooling across all conditions (H2b: $\hat{b}$ = `r exp1_descriptives$H2b$impressed$estimate` `r exp1_descriptives$H2b$impressed$ci`, p `r exp1_descriptives$H2b$impressed$p.value`).

# Experiment 2

In Experiment 1, exposure to impressive scientific content increased trust in the relevant scientific discipline, and the perceived competence of the relevant scientists. Experiment 2 sought to test whether these perceptions were at least partly independent of being able to recall the specific content that had induced them. Experiment 2 consisted of a ‘recall study’ and an ‘evaluation study.’ In the recall study, each participant was assigned to read the impressive version of one of the vignettes from Experiment 1 (Table \@ref(tab:exp1-stimuli)). As in Experiment 1, participants were asked about how impressed they were by the findings and whether they had changed their perception of the scientists’ competence and their trust in the scientific discipline.

In line with the findings of Experiment 1, we expected participants to have increased perceptions of competence and trust in scientists after having read the impressive texts:

H1a: Participants perceive scientists as more competent after having read an impressive text about their discipline's findings.

H1b: Participants trust a discipline more after having read an impressive text about the discipline's findings.

In Experiment 2, participants were also given a recall task: They were asked to rewrite the text of the vignette they had just read, from memory. We predicted that participants would not be able to recall all the information presented in the short vignettes right after having read them (see methods section for details):

H2: Participants are not able to recall all the information of the original texts.

This hypothesis, however, only tested whether people forgot any of the content, potentially including non-impressive content. To address this issue, we asked participants to select elements of the vignettes they found impressive (see Table \@ref(tab:knowledge-evaluation-grid)). We predicted that even for the subset of information that a participant said they found impressive, they forgot at least some of the content:

H3: Participants are not able to recall all the impressive information--as rated by themselves--contained in the original text.

H3 tests the hypothesis that participants immediately forget at least some of the information that has impressed them. However, it could be that participants in fact remember enough impressive information to justify the increase in perceived trust (in the discipline) and competence (in the scientists). To test whether that was the case, we conducted an evaluation study.

A new sample of participants was recruited, and they were randomly assigned to one of two conditions: In the 'original impressive text' condition, participants were assigned to read one of the two impressive texts of the recall study. In the 'recalled impressive text' condition, participants read one of the recall texts written by the participants of the recall study. We predicted that participants in the evaluation study would be less impressed by the texts recalled by the participants of the recall study, compared to the original impressive vignette texts (see Table \@ref(tab:exp1-stimuli)) and, accordingly, would have less positive perceptions of the scientists' competence and the trustworthiness of their discipline:

H4a: The texts produced by participants of the experiment as a result of the recall task will be less impressive than the original texts, as rated by participants of the **evaluation study**.

H4b: Participants of the **evaluation study** perceive scientists as more competent after having read the original texts, compared to after having read the texts produced by participants of the experiment as a result of the recall task.

H4c: Participants of the **evaluation study** trust a discipline more after having read the original texts, compared to after having read the texts produced by participants of the experiment as a result of the recall task.

## Methods

### Participants

In a power simulation (see OSF), we varied the sample size of the recall study and effect sizes (assuming the same effect size for all hypotheses in each scenario). For all simulations, a constant evaluation study sample size of 400 participants was assumed (only relevant for H4a, b and c). The power simulation suggested that a power level of 90% would be reached when assuming a medium effect size of 0.5 with 50 participants. Due to uncertainty about our assumptions, we recruited a sample of 203 participants for the recall study, and a sample of 406 participants for the evaluation study. Although this was not the focus of the simulation, the results showed that for a medium effect size of 0.5, a sample size of 400 for the evaluation study yielded statistical power of greater than 90% for all hypotheses based on this sample (H4a, b and c). All participants were from the UK, recruited via Prolific, and paid to complete the experiment.

The final sample comprised `r exp2_descriptives_everyone$n_subj` participants (four failed attention checks; `r exp2_descriptives_everyone$gender$female$n` female, `r exp2_descriptives_everyone$gender$male$n` male; $age_\text{mean}$: `r exp2_descriptives_everyone$age$mean`, $age_\text{sd}$: `r exp2_descriptives_everyone$age$sd`, $age_\text{median}$: `r exp2_descriptives_everyone$age$median`) for the recall study, and `r exp2_descriptives_validation$n_subj` participants for the evaluation study (seven failed attention checks; `r exp2_descriptives_validation$gender$female$n` female, `r exp2_descriptives_validation$gender$male$n` male; $age_\text{mean}$: `r exp2_descriptives_validation$age$mean`, $age_\text{sd}$: `r exp2_descriptives_validation$age$sd`, $age_\text{median}$: `r exp2_descriptives_validation$age$median`).

### Procedure

#### Recall study

In the recall study, after having consented to take part in the study and passing an attention check (see ESM), participants read the impressive version of one of the vignettes from Experiment 1 (Table \@ref(tab:exp1-stimuli)). After reading the text, as in Experiment 1, participants were asked about changes in their perception of the scientists’ competence, and trust in the scientists’ discipline. They were also asked about the impressiveness of the text they read. The order of these questions was randomized.

Next, as an open-ended question, participants were asked to recall as much information as they could of the texts they had just read. They were told that their texts would be read by future participants. To further motivate participants, they were also told that they would get a bonus for recalling (without external aids) accurate information. They were not told how much that bonus would be. We paid them 5p per point gained in the recall task (see methods for how these points were assigned). This way, participants could reach a maximum bonus of 0.8 pound for archaeology (0.05p x 2 points x 8 content elements) and 0.7 pound (0.05p x 2 points x 7 content elements) for entomology. After that, participants were presented with the evaluation grid that we used to assess the open answers from the recall task (see Table \@ref(tab:knowledge-evaluation-grid)). For each knowledge element, we asked participants to indicate whether they found it impressive or not ("Do you think this piece of information is impressive?" [Yes; No]). At the end of the recall study, participants were asked about their education level.

#### Evaluation study

After consenting to taking part in the study and passing an attention check (see ESM), participants read either one of the original vignettes, or a text produced as part of the recall task from the recall study. For those participants assigned to read a recall text, the text was randomly sampled (with replacement) from a pool of recall answers. Orthographic and grammatical mistakes in these texts were corrected with the help of ChatGPT beforehand. We had preregistered to sample among all recall answers from participants of the recall study. However, checking recall scores after the recall study and before launching the evaluation study, we found them to be very low on average (see ESM section \@ref(above-median)). To avoid having the answers of less motivated participants in our sample, we decided to only select answers that scored at least as well as the median in the recall measure of the evaluation study. This selection is conservative in that it makes it harder to confirm our predictions under H4.

After reading the text, just as in the recall study, participants were asked about changes in their perception of the scientists’ competence, trust in the scientists’ discipline, and the impressiveness of the text they read. The order of these questions was randomized. Finally, participants were asked about their education level.

### Materials

For Experiment 2, besides the texts recalled by the participants of the recall study, the impressive version of the stimuli used in Experiment 1 was used (see Table \@ref(tab:exp1-stimuli)).

```{r knowledge-evaluation-grid, echo=FALSE}
# Create the data frame with consistent row numbers
new_table <- data.frame(
  Archaeology = linebreak(c(
  "1. Archaeologists can determine whether someone was male or female from their bones.",
  "2. Archaeologists can determine how old someone was from their bones.",
  "3. Archaeologists can determine whether someone suffered from a range of diseases from their bones.",
  "4. Archaeologists can determine at what age someone stopped drinking their mother’s milk, based on the composition of their teeth.",
  "5. The nerve controlling breathing is larger in humans than in apes. The canal containing that nerve is also larger in humans and Neanderthals than in apes.",
  "6. The fact that the nerve controlling breathing is larger in humans is possibly due to the need for fine-grained control of breathing to speak.",
  "7. Archaeologists determined that most Neanderthals were right-handed, based on analysis of Neanderthals’ tools.",
  "8. Handedness is thought to be related to the evolution of language. This suggests that Neanderthals likely possessed a form of language."
)),
Entomology = linebreak(c(
  "1. Entomologists use special displays to present images to flies for extremely short periods (less than the blink of an eye).",
  "2. Entomologists can record how individual cells in flies’ brains react using electrodes.",
  "3. Entomologists use ultra-precise electron microscopy to examine flies’ eyes.",
  "4. Some flies can perceive images displayed for just three milliseconds. This duration is about ten times shorter than a single movie frame.",
  "5. Crickets have microscopic hairs situated on antenna-like organs at their rear. ",
  "6. Crickets' hairs are possibly the most sensitive organs in the animal kingdom. They react to changes in air motion with less energy than one photon.",
  "7. Entomologists measured how cricket hairs react to stimuli, using laser-Doppler velocimetry, which can detect extremely minute movements.",
  ""  # Placeholder to match vector length
))
)

# Output the table
# Create the basic table
if (knitr::is_latex_output() || knitr::is_html_output()) {
  # For LaTeX or HTML: Use kable, column_spec, and kable_styling
  table_output <- kableExtra::kable(
  new_table,
  col.names = c("Archaeology", "Entomology"),
  caption = "Recall evaluation grid",
  align = "l",
  booktabs = TRUE,
  format = "latex",
  longtable = TRUE,
  escape = FALSE # Keep LaTeX formatting
) %>%
  kable_paper(full_width = FALSE) %>% # Ensure consistentf styling
  column_spec(1, width = "20em") %>%  # Adjust column widths
  column_spec(2, width = "20em") %>%
  footnote(general = "For each knowledge element, participants could score a maximum of two points.")
} else {
  # For Word: Use papaja::apa_table()
  table_output <- papaja::apa_table(
    new_table,
    caption = "Recall evaluation grid",
    note = "For each knowledge element, participants could score a maximum of two points."
  )
}

table_output

```

#### Recall

As shown in Table \@ref(tab:knowledge-evaluation-grid), the texts were divided into a series of knowledge elements. For each participant, the extent to which they recalled each of the different elements was coded. A recall score based on how many of these elements they mentioned in their open-ended answer was then calculated.

The coding was done with the help of ChatGPT (see ESM section \@ref(gpt-prompt) for the exact prompt). The instructions were: to code 0 if a piece of knowledge was not mentioned or was mentioned with significant errors (e.g., writing “the nerve controlling fine hand movement is bigger in humans” instead of “the nerve controlling breathing is bigger in humans”); to code 1 if the piece of knowledge was mentioned, but some important elements were missing (e.g., writing “Archaeologists can determine at what age someone stopped drinking their mother’s milk” instead of "Archaeologists can determine at what age someone stopped drinking their mother’s milk from the composition of their teeth"), and/or there were some mistakes (e.g., writing “Archaeologists can determine at what age someone stopped drinking their mother’s milk, based on the bones” instead of “Archaeologists can determine at what age someone stopped drinking their mother’s milk based on the teeth”); to code 2 if the piece of knowledge was mentioned with all the main content, even if the participant had not used the precise technical words (e.g., “neanderthals,” “laser-Doppler velocimetry”) or had changed the phrasing in other ways. These instructions were intended to produce relatively generous recall scores.

Since the two vignettes contained a different number of total knowledge elements according to our evaluation grid (8 for archaeology, 7 for entomology), we used a relative measure for the final recall score, namely the share of obtained points among all possible points (possible range from 0 to 1, below the results are presented in percentages recalled for clarity). Practically, for all tests on recall, we computed a forgetting score (1-recall score), and tested whether it was statistically significantly different from zero.

To validate the scores assigned by ChatGPT, they were compared to scores assigned by two human coders for a subsample of 80 randomly chosen texts (half on archaeology, half on entomology). The human coders were unaware of the study context and the hypotheses. They were provided with the prompt given to ChatGPT. To measure the agreement between ChatGPT and the human coders, we calculated an intraclass correlation coefficient (ICC) for two-way designs with random raters [@tenhoveUpdatedGuidelinesSelecting2024]. Following the guidelines in @heymanBehavioralObservationCoding2014, we preregistered taking 0.7 as a threshold for acceptable reliability. In our sample, we observe an ICC of `r ICC$ICC` ($se$ = `r ICC$se`). Human coders and ChatGPT, on average, assigned exactly the same score in `r agreement_human_gpt` of all rating instances (compared to `r agreement_human` of exact agreement between the two human coders). Following our preregistration, we consider the high agreement as suggested by the ICC a validation of the use of ChatGPT.

#### Recall of impressive items

After giving their post-recall evaluation of scientists' competence and trust in the discipline, participants were presented with the evaluation grid shown in Table \@ref(tab:knowledge-evaluation-grid) for the respective discipline they had been randomized to see. For each element in the evaluation grid, they were asked whether they found it impressive or not. Then, for each participant, the recall score was computed just as described above, but only on the subset of those elements they had subjectively rated as impressive.

## Results and discussion

### Recall study

First, as a kind of validation check, we note that most participants declared finding all knowledge elements to be impressive ($mean_{\text{Archeology}}$ = `r exp2_descriptives_above_median$n_impressive$archeo$n_impressive_mean`, $median_{\text{Archeology}}$ = `r exp2_descriptives_above_median$n_impressive$archeo$n_impressive_median`, number of knowledge elements = 8; $mean_{\text{Entomology}}$ = `r exp2_descriptives_above_median$n_impressive$entom$n_impressive_mean`, $median_{\text{Entomology}}$ = `r exp2_descriptives_above_median$n_impressive$entom$n_impressive_median`, number of knowledge elements = 7).

For the first set of hypotheses, we tested whether the average scores for perceived change in competence and trust were significantly different from their respective scale midpoints (which corresponds to no change in perception). We first tested the outcome variables’ distributions for normality, using a Shapiro-Wilk test. In all cases, this test suggested that the data is considered non-normally distributed (p \< 0.05). Following the preregistration, we therefore did not run a default one-sample t-test, but used a Wilcoxon signed-rank test instead. H1a and H1b were both supported: After having read an impressive text about the findings of a scientific discipline, participants saw the scientists as more competent (H1a: median = `r exp2_descriptives_everyone$descriptive_stats$competence_median`, W = `r exp2_descriptives_everyone$results$H1a$statistic`, p `r exp2_descriptives_everyone$results$H1a$p.value`), and their discipline as more trustworthy (H1b: median = `r exp2_descriptives_everyone$descriptive_stats$trust_median`, W = `r exp2_descriptives_everyone$results$H1b$statistic`, p `r exp2_descriptives_everyone$results$H1b$p.value`).

Despite having been impressed, a first descriptive analysis suggested that participants seemed to recall only very little information ($mean$ = `r paste0(exp2_descriptives_everyone$descriptive_stats$knowledge_score_mean*100, "%")`, $sd$ = `r paste0(exp2_descriptives_everyone$descriptive_stats$knowledge_score_sd*100, "%")`, $median$ = `r paste0(exp2_descriptives_everyone$descriptive_stats$knowledge_score_median*100, "%")`). Given these low average scores, we opted for a more conservative approach to testing H2 and H3: We defined the median as a cut-off and selected only the 50% of participants who had the highest recall scores, removing participants who may have put in less effort. Since we hypothesized that participants would forget content, this selection made it less likely that the hypotheses would be confirmed. Even for the 50% participants with the best recall, both hypotheses were supported: Participants did not perfectly recall all information (H2: median = `r exp2_descriptives_above_median$descriptive_stats$knowledge_score_median`, W = `r exp2_descriptives_above_median$results$H2$statistic`, p `r exp2_descriptives_above_median$results$H2$p.value`) and they did not recall all information contained in the elements they judged as impressive themselves (H3: median = `r exp2_descriptives_above_median$descriptive_stats$impressive_knowledge_score_median`, W = `r exp2_descriptives_above_median$results$H2$statistic`, p `r exp2_descriptives_above_median$results$H2$p.value`)[^2].

[^2]: The two median values for all information and for the subset of impressive information are not not exactly the same, but very similar, because participants rated most knowledge elements as impressive, making the general recall score and the recall score for impressive elements very similar.

### Evaluation study

For H4a, b and c, trust, competence and impressiveness ratings were compared between the 'original impressive text' and the 'recalled impressive text' conditions of the evaluation study using independent sample t-tests. Participants who read one of the two original impressive texts reported being more impressed (H4a: $\hat{b}_{\text{Impressiveness}}$ = `r exp2_descriptives_validation$results$H4a$mean_difference`, t = `r exp2_descriptives_validation$results$H4a$statistic`, p `r exp2_descriptives_validation$results$H4a$p.value`; $mean_{\text{Original text}}$ = `r exp2_descriptives_validation$by_condition$original$impressiveness_mean`, $mean_{\text{Recalled text}}$ = `r exp2_descriptives_validation$by_condition$recall$impressiveness_mean`), rated the scientists of the respective discipline as more competent (H4b: $\hat{b}_{\text{Competence}}$ = `r exp2_descriptives_validation$results$H4b$mean_difference`, t = `r exp2_descriptives_validation$results$H4b$statistic`, p `r exp2_descriptives_validation$results$H4b$p.value`; $mean_{\text{Original text}}$ = `r exp2_descriptives_validation$by_condition$original$competence_mean`, $mean_{\text{Recalled text}}$ = `r exp2_descriptives_validation$by_condition$recall$competence_mean`), and had more trust in the respective discipline (H4c: $\hat{b}_{\text{Trust}}$ = `r exp2_descriptives_validation$results$H4c$mean_difference`, t = `r exp2_descriptives_validation$results$H4c$statistic`, p `r exp2_descriptives_validation$results$H4c$p.value`; $mean_{\text{Original text}}$ = `r exp2_descriptives_validation$by_condition$original$trust_mean`, $mean_{\text{Recalled text}}$ = `r exp2_descriptives_validation$by_condition$recall$trust_mean`) than participants who read one of the texts of the recall task.

# Discussion

From a perspective of cultural evolution, the fact that people around the globe tend to trust science is puzzling: people aren’t very interested in science, they struggle to understand its counterintuitive concepts, and yet they tend to trust it. A common explanation of trust in science, the deficit model, postulates that trust in science is grounded in knowledge and understanding---yet, people’s science literacy is generally low and at best weakly correlated to trust in science.

Recently, a cognitive account of trust in science has been proposed to make sense of this puzzle: the rational impression account. According to this account, people are impressed by science---in particular when they encounter it in their education---which leads them to trust it, but they then forget most of what they’ve learnt. In two experiments, we tested two central predictions of this account. Experiment 1 showed that participants perceived some scientific findings as more impressive than others. Exposure to the more impressive findings lead people to think of scientists as more competent and to trust science more. Experiment 2 showed that these impressions are formed even though participants forget most specific knowledge---including knowledge that can be assumed to have generated the impressions---almost immediately after reading it.

The present findings have practical implications: for people to generate positive impressions of science, they need to be exposed to science. This stresses the vital role of science communication and education in forming trust in science. The relationship between science communication and trust in science has already been explored in depth [e.g., @weingartScienceCommunicationIssue2016; for a recent review see @konigHowCommunicateScience2023], but we believe the rational impression account tested here makes a useful contribution. In particular, it suggests that, even though more understanding of the underlying methods is always preferable [@konigHowCommunicateScience2023], even a relatively superficial exposure to impressive findings can bolster trust in science. An important caveat is that impressive findings that haven’t yet gained the approval of the community might be particularly likely to backfire if people learn they have been disproven [on the importance of presenting a measure of consensus alongside scientific information, see @konigHowCommunicateScience2023].

The present experiments have a number of limitations: First, we did not determine which features exactly made some scientific findings more impressive than others to the participants. Uncovering these features would be both of theoretical and practical interest.

Second, in Experiment 2, we had to rely on an imperfect research design for testing whether impressions persisted more than specific knowledge. However, we believe that the study still presents convincing evidence that participants forgot some---or most of---the knowledge that generated their impressions, even immediately after forming the impressions. Two lines of evidence from Experiment 2 support this argument: First, the participants of the recall study rated the impressiveness of different passages after their recall. Even when looking only at the subjective sample of impressive information for each participant, they still forgot most of it. Second, a validation study with a different set of participants found that these participants rated the recalled texts from participants of the recall study as less impressive than the original impressive texts. If the participants of the recalled texts had only forgotten irrelevant bits of information, but not impressive bits, the expectation would have been no difference.

As a third limitation, our studies were conducted on convenience samples recruited in a single country (the UK). Fourth, the studies covered a very short time frame. While we can show that participants almost immediately forget about impressive content, it is not clear from our study for how long the impressions persist [although in other contexts impressions formed on the basis of a much more superficial exposure have been shown to last for months, @gunaydinImpressionsBasedPortrait2017]. Future studies could extend our findings to other populations and to longer time frames.

Besides providing support for the rational account of trust in science, we hope that our findings, and the research question that prompted them, will draw more research to the grounding of trust in science in cultural evolution.

### Data availability

Data for all experiments and the simulations is available on the OSF project page (<https://osf.io/j3bk4/overview>). Note that on the project page, all materials related to what is referred to as "Experiment 1" in this paper are stored under "experiment_2", and all materials related to "Experiment 2" in this paper are stored under "experiment_4". This numbering is due to the original order in which experiments for this project were conducted. For a detailed report on the other experiments conducted as part of this project, see the ESM.

### Code availability

The code used to create all results (including tables and figures) of this manuscript is also available on the OSF project page (<https://osf.io/j3bk4/overview>).

### Competing interest

The authors declare having no competing interests.

\FloatBarrier

# References

::: {#refs}
:::

\newpage

# (APPENDIX) Appendix {.unnumbered}

```{r child = "appendix_gpt-prompt.Rmd"}
```

\clearpage

```{r child = "appendix_above-median-recall.Rmd"}
```

\clearpage

```{r child = "appendix_stability.Rmd"}
```

\clearpage

```{r child = "appendix_inter-coder.Rmd"}
```

\clearpage

```{r child = "appendix_exp1.Rmd"}
```

\clearpage

```{r child = "appendix_exp2.Rmd"}
```

\clearpage

```{r child = "appendix_exp3.Rmd"}
```

\clearpage

```{r child = "appendix_exp4.Rmd"}
```
